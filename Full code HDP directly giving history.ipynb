{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "import math\n",
    "import cv2\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "from datetime import datetime, timedelta\n",
    "from skimage.segmentation import slic\n",
    "from skimage.segmentation import mark_boundaries\n",
    "from skimage.segmentation import find_boundaries\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from numpy.linalg import inv\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_reading(file_path):\n",
    "    #reading annotations file\n",
    "    ann_file = open(file_path,\"r\") #opening file in read mode only\n",
    "    strings = [x.strip() for x in ann_file.readlines()]\n",
    "    stimes=[]\n",
    "    etimes=[]\n",
    "    for i in range(len(strings)):\n",
    "        s1,s2=strings[i].split(\"-\")\n",
    "        stimes.append(s1.strip())\n",
    "        etimes.append(s2.strip())\n",
    "    return stimes,etimes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare(stime,etime,abnormal_stimes,abnormal_etimes):\n",
    "    length = len(abnormal_stimes)\n",
    "    for i in range(length):\n",
    "        t1 = datetime.strptime(abnormal_stimes[i], '%M:%S').time()\n",
    "        t2 = datetime.strptime(abnormal_etimes[i], '%M:%S').time()\n",
    "        obj1 = timedelta(hours=t1.hour, minutes=t1.minute, seconds=t1.second)\n",
    "        obj2 = timedelta(hours=t2.hour, minutes=t2.minute, seconds=t2.second)\n",
    "        if (stime >= obj1 and etime <= obj2) or (stime < obj1 and etime > obj1) or (stime < obj2 and etime > obj2):\n",
    "            return 1\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class labeling_objects:\n",
    "    def __init__(self,clip_no,stime,etime,label,interest_points,segments):\n",
    "        self.clip_no = clip_no\n",
    "        self.stime = stime\n",
    "        self.etime = etime\n",
    "        self.label = label\n",
    "        self.interest_points = interest_points\n",
    "        self.segments = segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def active_superpixels(flow,segments,frame):\n",
    "    no_of_superpixels = segments.max()+1\n",
    "    #print(no_of_superpixels)\n",
    "    height,width = segments.shape\n",
    "    count = [0]*no_of_superpixels\n",
    "    points = []\n",
    "    threshold = [5.00000000e-05,5.00000000e-05]\n",
    "    \n",
    "    for i in range(height):\n",
    "        for j in range(width):\n",
    "            if abs(flow[i][j][0]) > threshold[0] or abs(flow[i][j][1]) > threshold[0]:\n",
    "                points.append([i,j])\n",
    "    \n",
    "    points = np.asarray(points)\n",
    "    \n",
    "    for index in range(len(points)):\n",
    "        seg = segments[points[index][0]][points[index][1]]\n",
    "        count[seg]+= 1\n",
    "\n",
    "    active_superpixels = [True]*no_of_superpixels\n",
    "    for i in range(no_of_superpixels):\n",
    "        total_count = np.count_nonzero(segments==i) #counting total no of pixels in superpixel i\n",
    "        if count[i] >= 0.4*total_count:\n",
    "            active_superpixels[i] = False\n",
    "\n",
    "    return active_superpixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "video_path = 'E:\\\\Study\\\\Sem Project\\\\Data\\\\traffic-junction.avi'\n",
    "ann_file_path = \"E:\\\\Study\\\\Sem Project\\\\Data\\\\abnormal_times.txt\"\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# params for ShiTomasi corner detection\n",
    "feature_params = dict( maxCorners = 500,   # How many pts. to locate\n",
    "                       qualityLevel = 0.1,  # b/w 0 & 1, min. quality below which everyone is rejected\n",
    "                       minDistance = 7,   # Min eucledian distance b/w corners detected\n",
    "                       blockSize = 3 ) # Size of an average block for computing a derivative covariation matrix over each pixel neighborhood\n",
    "\n",
    "# Parameters for lucas kanade optical flow\n",
    "lk_params = dict( winSize  = (15,15),  # size of the search window at each pyramid level\n",
    "                  maxLevel = 2,   #  0, pyramids are not used (single level), if set to 1, two levels are used, and so on\n",
    "                  criteria = (cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 0.03))\n",
    "\n",
    "''' Criteria : Termination criteria for iterative search algorithm.\n",
    "    after maxcount { Criteria_Count } : no. of max iterations.\n",
    "    or after { Criteria Epsilon } : search window moves by less than this epsilon '''\n",
    "\n",
    "total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "clip = 0\n",
    "count = 0  # for the frame count\n",
    "n = 50  # Frames refresh rate for feature generation\n",
    "\n",
    "interest_points = []\n",
    "history_all_clips = [] # to track the history of frames.\n",
    "\n",
    "#for labeling the clips \n",
    "abnormal_stimes,abnormal_etimes = file_reading(ann_file_path)\n",
    "secs=0\n",
    "label_objects_array = []\n",
    "frames = []\n",
    "active_super_pixels_array = []\n",
    "seg_array = []\n",
    "\n",
    "while True:\n",
    "    ret,frame = cap.read()\n",
    "    if frame is None:\n",
    "        break\n",
    "    frames.append(frame)\n",
    "    count+=1\n",
    "    \n",
    "    #cutting the clip, finding the interest points for last frame\n",
    "    if count%n == 0 or (len(frames)!=0 and (count == total_frames)):\n",
    "        if(len(frames)==n):\n",
    "            secs = secs+2\n",
    "            stime = timedelta(seconds = secs-2)\n",
    "            etime = timedelta(seconds = secs)\n",
    "            label = compare(stime,etime,abnormal_stimes,abnormal_etimes)\n",
    "        else:\n",
    "            secs = secs+1\n",
    "            stime = timedelta(seconds = secs-1)\n",
    "            etime = timedelta(seconds = secs)\n",
    "            label = compare(stime,etime,abnormal_stimes,abnormal_etimes)\n",
    "        \n",
    "        len_frame = len(frames)\n",
    "        old_frame = frames[len_frame-1]\n",
    "        # Convert to Grey Frame\n",
    "        old_gray = cv2.cvtColor(old_frame, cv2.COLOR_BGR2GRAY)\n",
    "        segments = slic(old_frame, n_segments = 144, sigma = 5,compactness = 25)\n",
    "        seg_array.append(segments)\n",
    "        mask = np.zeros_like(old_frame)\n",
    "        \n",
    "        #features for ending frame\n",
    "        p0 = cv2.goodFeaturesToTrack(old_gray, mask=None, **feature_params)\n",
    "        t = np.ones(shape = (len(p0),1))\n",
    "        \n",
    "        label_objects_array.append(labeling_objects(clip,stime,etime,label,p0[t==1],segments.copy()))\n",
    "        clip += 1\n",
    "        \n",
    "        history_clip = defaultdict(list)\n",
    "        \n",
    "        for i in range(len(frames)-1,0,-1):\n",
    "            # calculate optical flow\n",
    "            frame_new = frames[i]\n",
    "            frame_gray = cv2.cvtColor(frame_new,cv2.COLOR_BGR2GRAY)\n",
    "            p1, st, err = cv2.calcOpticalFlowPyrLK(old_gray, frame_gray, p0, None, **lk_params)\n",
    "            if i==len(frames)-1:\n",
    "                flow = cv2.calcOpticalFlowFarneback(old_gray,frame_gray, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "                flow=np.asarray(flow)\n",
    "                active_super_pixels_array.append(active_superpixels(flow,segments,old_frame))\n",
    "            # Select good points\n",
    "            good_new = p1[st==1]\n",
    "            good_old = p0[st==1]\n",
    "\n",
    "            # draw the tracks\n",
    "            for j,(new,old) in enumerate(zip(good_new,good_old)):\n",
    "                if i == len(frames)-1:\n",
    "                    history_clip[j].append(old)\n",
    "                    history_clip[j].append(new)\n",
    "                else:\n",
    "                    history_clip[j].append(new)\n",
    "\n",
    "            # Now update the previous frame and previous points\n",
    "            old_gray = frame_gray.copy()\n",
    "            p0 = good_new.reshape(-1,1,2)\n",
    "        frames = []\n",
    "        history_all_clips.append(history_clip)\n",
    "\n",
    "print(\"total frames \",count)\n",
    "print(\"total clips\",clip)\n",
    "# release and destroy all windows\n",
    "cv2.destroyAllWindows()\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def region_formation(image):\n",
    "    shape = image.shape\n",
    "    x_val = shape[0]//3\n",
    "    y_val = shape[1]//3\n",
    "    regions_arr = image.reshape(int(shape[0]/x_val), x_val, -1, y_val).swapaxes(1,2).reshape(-1, x_val, y_val)\n",
    "    return regions_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def small_length(val,arr):\n",
    "    small_index = 0\n",
    "    small_count = math.inf\n",
    "    for i in range(len(arr)):\n",
    "        if val in arr[i]:\n",
    "            if small_count > len(arr[i]):\n",
    "                small_count = len(arr[i])\n",
    "                small_index = i\n",
    "    return small_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def in_list(c, region_list):\n",
    "    for i in range(len(region_list)):\n",
    "        if c in region_list[i]:\n",
    "            return i\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def region_and_superpixels(segments):\n",
    "    #dividing the entire frame into regions assigning each pixel location to region number\n",
    "    #output: regions_ar- contains the information of superpixels belongs to that region\n",
    "    #        region_segments - for a particular pixel location its associated region number.\n",
    "    regions = region_formation(segments)\n",
    "    region_index = defaultdict(set)\n",
    "    x,m,n = regions.shape\n",
    "    for i in range(x):\n",
    "        for j in range(m):\n",
    "            for k in range(n):\n",
    "                region_index[i].add(regions[i][j][k])\n",
    "    regions_ar = defaultdict(list)\n",
    "    for i in range(len(region_index)):\n",
    "        regions_ar[i]=list(region_index[i])\n",
    "    for i in range(len(regions_ar)):\n",
    "        for j in range(len(regions_ar[i])):\n",
    "            region_num = small_length(regions_ar[i][j],regions)\n",
    "            for ind in range(len(regions_ar)):\n",
    "                if ind!=region_num and ind!=i:\n",
    "                    if regions_ar[i][j] in regions_ar[ind]:\n",
    "                        regions_ar[ind].remove(regions_ar[i][j])\n",
    "                        \n",
    "    region_segments = np.zeros_like(segments)\n",
    "    for i in range(len(segments)):\n",
    "        for j in range(len(segments[i])):\n",
    "            region_segments[i][j] = in_list(segments[i][j],regions_ar)\n",
    "    \n",
    "    return regions_ar,region_segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_segments_array = []\n",
    "for i in range(len(label_objects_array)):\n",
    "    regions_ar,region_segments = region_and_superpixels(label_objects_array[i].segments)\n",
    "    region_segments_array.append(region_segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "codebook_all_clips = []\n",
    "for i in range(len(history_all_clips)):\n",
    "    codebook_region = defaultdict(list)\n",
    "    for j in range(len(history_all_clips[i])):\n",
    "        history = history_all_clips[i][j]\n",
    "        flow_value =[]\n",
    "        #superpixel value \n",
    "        suppix = label_objects_array[i].segments[int(history[0][1])][int(history[0][0])]\n",
    "        region = region_segments_array[i][int(history[0][1])][int(history[0][0])]\n",
    "        for k in range(len(history)-1):\n",
    "            flow_value.append([history[k][0]-history[k+1][0],history[k][1]-history[k+1][1]])\n",
    "        flow_value = np.asarray(flow_value)\n",
    "        mag = cv2.cartToPolar(flow_value[:,0],flow_value[:,1])[0]\n",
    "        mag = mag.flatten()\n",
    "        codebook_region[region].append(np.asarray(mag))\n",
    "    region_array = np.asarray(list(codebook_region.values()))\n",
    "    codebook_all_clips.append(region_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "from scipy.special import gammaln\n",
    "\n",
    "class DefaultDict(dict):\n",
    "    def __init__(self, v):\n",
    "        self.v = v\n",
    "        dict.__init__(self)\n",
    "    def __getitem__(self, k):\n",
    "        return dict.__getitem__(self, k) if k in self else self.v\n",
    "    def update(self, d):\n",
    "        dict.update(self, d)\n",
    "        return self\n",
    "\n",
    "class HDPLDA:\n",
    "    def __init__(self, alpha, beta, gamma, docs, V):\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.gamma = gamma\n",
    "        self.V = V\n",
    "        self.M = len(docs)\n",
    "\n",
    "        # t : table index for document j\n",
    "        #     t=0 means to draw a new table\n",
    "        self.using_t = [[0] for j in range(self.M)]\n",
    "\n",
    "        # k : dish(topic) index\n",
    "        #     k=0 means to draw a new dish\n",
    "        self.using_k = [0]\n",
    "\n",
    "        self.x_ji = docs # vocabulary for each document and term\n",
    "        self.k_jt = [numpy.zeros(1 ,dtype=int) for j in range(self.M)]   # topics of document and table\n",
    "        self.n_jt = [numpy.zeros(1 ,dtype=int) for j in range(self.M)]   # number of terms for each table of document\n",
    "        self.n_jtv = [[None] for j in range(self.M)]\n",
    "\n",
    "        self.m = 0\n",
    "        self.m_k = numpy.ones(1 ,dtype=int)  # number of tables for each topic\n",
    "        self.n_k = numpy.array([self.beta * self.V]) # number of terms for each topic ( + beta * V )\n",
    "        self.n_kv = [DefaultDict(0)]            # number of terms for each topic and vocabulary ( + beta )\n",
    "\n",
    "        # table for each document and term (-1 means not-assigned)\n",
    "        self.t_ji = [numpy.zeros(len(x_i), dtype=int) - 1 for x_i in docs]\n",
    "                    \n",
    "\n",
    "    def worddist(self):\n",
    "        \"\"\"return topic-word distribution without new topic\"\"\"\n",
    "        return [DefaultDict(self.beta / self.n_k[k]).update(\n",
    "            (v, n_kv / self.n_k[k]) for v, n_kv in self.n_kv[k].items())\n",
    "                for k in self.using_k if k != 0]\n",
    "\n",
    "    def docdist(self):\n",
    "        \"\"\"return document-topic distribution with new topic\"\"\"\n",
    "\n",
    "        # am_k = effect from table-dish assignment\n",
    "        am_k = numpy.array(self.m_k, dtype=float)\n",
    "        am_k[0] = self.gamma\n",
    "        am_k *= self.alpha / am_k[self.using_k].sum()\n",
    "\n",
    "        theta = []\n",
    "        for j, n_jt in enumerate(self.n_jt):\n",
    "            p_jk = am_k.copy()\n",
    "            for t in self.using_t[j]:\n",
    "                if t == 0: continue\n",
    "                k = self.k_jt[j][t]\n",
    "                p_jk[k] += n_jt[t]\n",
    "            p_jk = p_jk[self.using_k]\n",
    "            theta.append(p_jk / p_jk.sum())\n",
    "\n",
    "        return numpy.array(theta)\n",
    "    \n",
    "    def dump(self, disp_x=False):\n",
    "        if disp_x: print(\"x_ji:\", self.x_ji)\n",
    "        print(\"using_t:\", self.using_t)\n",
    "        print(\"t_ji:\", self.t_ji)\n",
    "        print(\"using_k:\", self.using_k)\n",
    "        print(\"k_jt:\", self.k_jt)\n",
    "        print(\"----\")\n",
    "        print(\"n_jt:\", self.n_jt)\n",
    "        print(\"n_jtv:\", self.n_jtv)\n",
    "        print(\"n_k:\", self.n_k)\n",
    "        print(\"n_kv:\", self.n_kv)\n",
    "        print(\"m:\", self.m)\n",
    "        print(\"m_k:\", self.m_k)\n",
    "\n",
    "\n",
    "    def sampling_t(self, j, i):\n",
    "        \"\"\"sampling t (table) from posterior\"\"\"\n",
    "        self.leave_from_table(j, i)\n",
    "\n",
    "        v = self.x_ji[j][i]\n",
    "        f_k = self.calc_f_k(v)\n",
    "        assert f_k[0] == 0 # f_k[0] is a dummy and will be erased\n",
    "\n",
    "        # sampling from posterior p(t_ji=t)\n",
    "        p_t = self.calc_table_posterior(j, f_k)\n",
    "        if len(p_t) > 1 and p_t[1] < 0: self.dump()\n",
    "        t_new = self.using_t[j][numpy.random.multinomial(1, p_t).argmax()]\n",
    "        if t_new == 0:\n",
    "            p_k = self.calc_dish_posterior_w(f_k)\n",
    "            k_new = self.using_k[numpy.random.multinomial(1, p_k).argmax()]\n",
    "            if k_new == 0:\n",
    "                k_new = self.add_new_dish()\n",
    "            t_new = self.add_new_table(j, k_new)\n",
    "\n",
    "        # increase counters\n",
    "        self.seat_at_table(j, i, t_new)\n",
    "        return p_t\n",
    "\n",
    "    def leave_from_table(self, j, i):\n",
    "        t = self.t_ji[j][i]\n",
    "        if t  > 0:\n",
    "            k = self.k_jt[j][t]\n",
    "            assert k > 0\n",
    "\n",
    "            # decrease counters\n",
    "            v = self.x_ji[j][i]\n",
    "            self.n_kv[k][v] -= 1\n",
    "            self.n_k[k] -= 1\n",
    "            self.n_jt[j][t] -= 1\n",
    "            self.n_jtv[j][t][v] -= 1\n",
    "\n",
    "            if self.n_jt[j][t] == 0:\n",
    "                self.remove_table(j, t)\n",
    "\n",
    "    def remove_table(self, j, t):\n",
    "        \"\"\"remove the table where all guests are gone\"\"\"\n",
    "        k = self.k_jt[j][t]\n",
    "        self.using_t[j].remove(t)\n",
    "        self.m_k[k] -= 1\n",
    "        self.m -= 1\n",
    "        assert self.m_k[k] >= 0\n",
    "        if self.m_k[k] == 0:\n",
    "            # remove topic (dish) where all tables are gone\n",
    "            self.using_k.remove(k)\n",
    "\n",
    "    def calc_f_k(self, v):\n",
    "        return [n_kv[v] for n_kv in self.n_kv] / self.n_k\n",
    "\n",
    "    def calc_table_posterior(self, j, f_k):\n",
    "        using_t = self.using_t[j]\n",
    "        p_t = self.n_jt[j][using_t] * f_k[self.k_jt[j][using_t]]\n",
    "        p_x_ji = numpy.inner(self.m_k, f_k) + self.gamma / self.V\n",
    "        p_t[0] = p_x_ji * self.alpha / (self.gamma + self.m)\n",
    "        #print(\"un-normalized p_t = \", p_t)\n",
    "        return p_t / p_t.sum()\n",
    "\n",
    "    def seat_at_table(self, j, i, t_new):\n",
    "        assert t_new in self.using_t[j]\n",
    "        self.t_ji[j][i] = t_new\n",
    "        self.n_jt[j][t_new] += 1\n",
    "\n",
    "        k_new = self.k_jt[j][t_new]\n",
    "        self.n_k[k_new] += 1\n",
    "\n",
    "        v = self.x_ji[j][i]\n",
    "        self.n_kv[k_new][v] += 1\n",
    "        self.n_jtv[j][t_new][v] += 1\n",
    "\n",
    "    # Assign guest x_ji to a new table and draw topic (dish) of the table\n",
    "    def add_new_table(self, j, k_new):\n",
    "        assert k_new in self.using_k\n",
    "        for t_new, t in enumerate(self.using_t[j]):\n",
    "            if t_new != t: break\n",
    "        else:\n",
    "            t_new = len(self.using_t[j])\n",
    "            self.n_jt[j].resize(t_new+1)\n",
    "            self.k_jt[j].resize(t_new+1)\n",
    "            self.n_jtv[j].append(None)\n",
    "\n",
    "        self.using_t[j].insert(t_new, t_new)\n",
    "        self.n_jt[j][t_new] = 0  # to make sure\n",
    "        self.n_jtv[j][t_new] = DefaultDict(0)\n",
    "\n",
    "        self.k_jt[j][t_new] = k_new\n",
    "        self.m_k[k_new] += 1\n",
    "        self.m += 1\n",
    "\n",
    "        return t_new\n",
    "\n",
    "    def calc_dish_posterior_w(self, f_k):\n",
    "        \"calculate dish(topic) posterior when one word is removed\"\n",
    "        p_k = (self.m_k * f_k)[self.using_k]\n",
    "        p_k[0] = self.gamma / self.V\n",
    "        return p_k / p_k.sum()\n",
    "\n",
    "\n",
    "\n",
    "    def sampling_k(self, j, t):\n",
    "        \"\"\"sampling k (dish=topic) from posterior\"\"\"\n",
    "        self.leave_from_dish(j, t)\n",
    "\n",
    "        # sampling of k\n",
    "        p_k = self.calc_dish_posterior_t(j, t)\n",
    "        k_new = self.using_k[numpy.random.multinomial(1, p_k).argmax()]\n",
    "        if k_new == 0:\n",
    "            k_new = self.add_new_dish()\n",
    "\n",
    "        self.seat_at_dish(j, t, k_new)\n",
    "\n",
    "    def leave_from_dish(self, j, t):\n",
    "        \"\"\"\n",
    "        This makes the table leave from its dish and only the table counter decrease.\n",
    "        The word counters (n_k and n_kv) stay.\n",
    "        \"\"\"\n",
    "        k = self.k_jt[j][t]\n",
    "        assert k > 0\n",
    "        assert self.m_k[k] > 0\n",
    "        self.m_k[k] -= 1\n",
    "        self.m -= 1\n",
    "        if self.m_k[k] == 0:\n",
    "            self.using_k.remove(k)\n",
    "            self.k_jt[j][t] = 0\n",
    "\n",
    "    def calc_dish_posterior_t(self, j, t):\n",
    "        \"calculate dish(topic) posterior when one table is removed\"\n",
    "        k_old = self.k_jt[j][t]     # it may be zero (means a removed dish)\n",
    "        #print(\"V=\", self.V, \"beta=\", self.beta, \"n_k=\", self.n_k)\n",
    "        Vbeta = self.V * self.beta\n",
    "        n_k = self.n_k.copy()\n",
    "        n_jt = self.n_jt[j][t]\n",
    "        n_k[k_old] -= n_jt\n",
    "        n_k = n_k[self.using_k]\n",
    "        log_p_k = numpy.log(self.m_k[self.using_k]) + gammaln(n_k) - gammaln(n_k + n_jt)\n",
    "        log_p_k_new = numpy.log(self.gamma) + gammaln(Vbeta) - gammaln(Vbeta + n_jt)\n",
    "        #print(\"log_p_k_new+=gammaln(\",Vbeta,\") - gammaln(\",Vbeta + n_jt,\")\")\n",
    "\n",
    "        gammaln_beta = gammaln(self.beta)\n",
    "        for w, n_jtw in self.n_jtv[j][t].items():\n",
    "            assert n_jtw >= 0\n",
    "            if n_jtw == 0: continue\n",
    "            n_kw = numpy.array([n.get(w, self.beta) for n in self.n_kv])\n",
    "            n_kw[k_old] -= n_jtw\n",
    "            n_kw = n_kw[self.using_k]\n",
    "            n_kw[0] = 1 # dummy for logarithm's warning\n",
    "            if numpy.any(n_kw <= 0): print(n_kw) # for debug\n",
    "            log_p_k += gammaln(n_kw + n_jtw) - gammaln(n_kw)\n",
    "            log_p_k_new += gammaln(self.beta + n_jtw) - gammaln_beta\n",
    "            #print(\"log_p_k_new+=gammaln(\",self.beta + n_jtw,\") - gammaln(\",self.beta,\"), w=\",w)\n",
    "        log_p_k[0] = log_p_k_new\n",
    "        #print(\"un-normalized p_k = \", numpy.exp(log_p_k))\n",
    "        p_k = numpy.exp(log_p_k - log_p_k.max())\n",
    "        return p_k / p_k.sum()\n",
    "\n",
    "    def seat_at_dish(self, j, t, k_new):\n",
    "        self.m += 1\n",
    "        self.m_k[k_new] += 1\n",
    "\n",
    "        k_old = self.k_jt[j][t]     # it may be zero (means a removed dish)\n",
    "        if k_new != k_old:\n",
    "            self.k_jt[j][t] = k_new\n",
    "\n",
    "            n_jt = self.n_jt[j][t]\n",
    "            if k_old != 0: self.n_k[k_old] -= n_jt\n",
    "            self.n_k[k_new] += n_jt\n",
    "            for v, n in self.n_jtv[j][t].items():\n",
    "                if k_old != 0: self.n_kv[k_old][v] -= n\n",
    "                self.n_kv[k_new][v] += n\n",
    "\n",
    "\n",
    "    def add_new_dish(self):\n",
    "        \"This is commonly used by sampling_t and sampling_k.\"\n",
    "        for k_new, k in enumerate(self.using_k):\n",
    "            if k_new != k: break\n",
    "        else:\n",
    "            k_new = len(self.using_k)\n",
    "            if k_new >= len(self.n_kv):\n",
    "                self.n_k = numpy.resize(self.n_k, k_new + 1)\n",
    "                self.m_k = numpy.resize(self.m_k, k_new + 1)\n",
    "                self.n_kv.append(None)\n",
    "            assert k_new == self.using_k[-1] + 1\n",
    "            assert k_new < len(self.n_kv)\n",
    "\n",
    "        self.using_k.insert(k_new, k_new)\n",
    "        self.n_k[k_new] = self.beta * self.V\n",
    "        self.m_k[k_new] = 0\n",
    "        self.n_kv[k_new] = DefaultDict(self.beta)\n",
    "        return k_new\n",
    "    \n",
    "    def inference(self):\n",
    "        weights = []\n",
    "        for j, x_i in enumerate(self.x_ji):\n",
    "            p_t = []\n",
    "            for i in range(len(x_i)):\n",
    "                p_t = self.sampling_t(j, i)\n",
    "            weights.extend(p_t)\n",
    "        for j in range(self.M):\n",
    "            for t in self.using_t[j]:\n",
    "                if t != 0: self.sampling_k(j, t)\n",
    "        return np.asarray(weights).flatten()\n",
    "    \n",
    "    def hdplda_learning(self,iteration):\n",
    "        for i in range(iteration):\n",
    "            weights = self.inference()\n",
    "        return weights       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class data_objects:\n",
    "    def __init__(self,clip_no,stime,etime,weights_data,label):\n",
    "        self.clip_no = clip_no\n",
    "        self.stime = stime\n",
    "        self.etime = etime\n",
    "        self.weights_data = weights_data\n",
    "        self.label = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_object_array=[]\n",
    "loa = label_objects_array\n",
    "for i in range(len(codebook_all_clips)):\n",
    "    print(\"clip no \",i)\n",
    "    clip_weights = []\n",
    "    for j in range(len(codebook_all_clips[i])):\n",
    "        #print(\"region no\",j)\n",
    "        if len(codebook_all_clips[i][j])!=0:\n",
    "            hdplda = HDPLDA(1,1,1, codebook_all_clips[i][j],(12*12)+1)\n",
    "            weights = hdplda.hdplda_learning(3)\n",
    "            clip_weights.extend(weights)\n",
    "    data_object_array.append(data_objects(loa[i].clip_no,loa[i].stime,loa[i].etime,np.asarray(clip_weights).flatten(),loa[i].label)) \n",
    "    print(\"weights\",len(clip_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_data_objects = data_object_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_curve(fpr, tpr):\n",
    "    plt.plot(fpr, tpr, color='orange', label='ROC')\n",
    "    plt.plot([0, 1], [0, 1], color='darkblue', linestyle='--')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalizing weights.\n",
    "for i in range(len(total_data_objects)):\n",
    "    sum = np.sum(total_data_objects[i].weights_data)\n",
    "    for j in range(len(total_data_objects[i].weights_data)):\n",
    "        total_data_objects[i].weights_data[j] = total_data_objects[i].weights_data[j] / sum\n",
    "    total_data_objects[i].weights_data = np.asarray(total_data_objects[i].weights_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = len(total_data_objects[0].weights_data)\n",
    "for i in range(len(total_data_objects)):\n",
    "    if max_len < len(total_data_objects[i].weights_data):\n",
    "        max_len = len(total_data_objects[i].weights_data)\n",
    "print(\"max len\",max_len)\n",
    "\n",
    "for i in range(len(total_data_objects)):\n",
    "    for j in range(max_len - len(total_data_objects[i].weights_data)):\n",
    "        total_data_objects[i].weights_data = np.append(total_data_objects[i].weights_data,0)\n",
    "\n",
    "for i in range(len(total_data_objects)):\n",
    "    for j in range(len(total_data_objects[i].weights_data)):\n",
    "        if total_data_objects[i].weights_data[j] == 0:\n",
    "            total_data_objects[i].weights_data[j] = np.mean([total_data_objects[ind].weights_data[j] for ind in range(len(total_data_objects))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dividing the total dataset into 60% training and 40% test\n",
    "total_no = len(total_data_objects)\n",
    "print(\"total objects=\",total_no)\n",
    "train_no=round(0.6*total_no)\n",
    "print(\"train objects=\",train_no)\n",
    "test_no=round(0.4*total_no)\n",
    "print(\"test objects=\",test_no)\n",
    "\n",
    "X_train=[]\n",
    "X_test=[]\n",
    "Y_train=[]\n",
    "Y_test=[]\n",
    "\n",
    "np.random.shuffle(total_data_objects)\n",
    "for i in range(train_no):\n",
    "    X_train.append(np.asarray(total_data_objects[i].weights_data))\n",
    "    Y_train.append(total_data_objects[i].label)\n",
    "for i in range(train_no,total_no):\n",
    "    X_test.append(total_data_objects[i].weights_data)\n",
    "    Y_test.append(total_data_objects[i].label)\n",
    "\n",
    "X_test=np.array(X_test)\n",
    "X_train=np.asarray(X_train)\n",
    "\n",
    "Y_test=np.asarray(Y_test)\n",
    "Y_train=np.asarray(Y_train)\n",
    "Y_test=Y_test\n",
    "Y_train=Y_train\n",
    "Y_test=Y_test.reshape(-1,1)\n",
    "Y_train=Y_train.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"shape \",X_train.shape)\n",
    "print(\"shape \",Y_train.shape)\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(X_train,Y_train)\n",
    "pred_labels=gnb.predict(X_test)\n",
    "print(\"labels predicted\",len(pred_labels))\n",
    "print(Y_test.reshape(1,-1))\n",
    "print(pred_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#performance\n",
    "print(\"accuracy =\",accuracy_score(pred_labels,Y_test)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(pred_labels,Y_test)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc = roc_auc_score(Y_test,pred_labels)\n",
    "print('AUC: %.2f' % auc)\n",
    "fpr, tpr, thresholds = roc_curve(Y_test,pred_labels)\n",
    "plot_roc_curve(fpr, tpr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
